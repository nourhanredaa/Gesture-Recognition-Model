{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importin Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MediaPipe Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_hand = list(range(21))\n",
    "filtered_pose = [0, 2, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
    "\n",
    "HAND_NUM = len(filtered_hand)\n",
    "POSE_NUM = len(filtered_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands = mp.solutions.hands.Hands()\n",
    "pose = mp.solutions.pose.Pose()\n",
    "\n",
    "def get_all_landmarks(frame):\n",
    "    \n",
    "    all_landmarks = np.zeros((HAND_NUM * 2 + POSE_NUM, 3))\n",
    "    \n",
    "    def get_hands(frame):\n",
    "        results_hands = hands.process(frame)\n",
    "        if results_hands.multi_hand_landmarks:\n",
    "            for i, hand_landmarks in enumerate(results_hands.multi_hand_landmarks):\n",
    "                if results_hands.multi_handedness[i].classification[0].index == 0: \n",
    "                    all_landmarks[:HAND_NUM, :] = np.array(\n",
    "                        [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]) # right\n",
    "                else:\n",
    "                    all_landmarks[HAND_NUM:HAND_NUM * 2, :] = np.array(\n",
    "                        [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]) # left\n",
    "\n",
    "    def get_pose(frame):\n",
    "        results_pose = pose.process(frame)\n",
    "        if results_pose.pose_landmarks:\n",
    "            all_landmarks[HAND_NUM * 2:HAND_NUM * 2 + POSE_NUM, :] = np.array(\n",
    "                [(lm.x, lm.y, lm.z) for lm in results_pose.pose_landmarks.landmark])[filtered_pose]\n",
    "        \n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        executor.submit(get_hands, frame)\n",
    "        executor.submit(get_pose, frame)\n",
    "\n",
    "    return all_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hands_landmarks(frame):\n",
    "    results_hands = hands.process(frame)\n",
    "    hand_lm = np.zeros((HAND_NUM * 2, 3))\n",
    "    if results_hands.multi_hand_landmarks:\n",
    "        for i, hand_landmarks in enumerate(results_hands.multi_hand_landmarks):\n",
    "            if results_hands.multi_handedness[i].classification[0].index == 0: \n",
    "                hand_lm[:HAND_NUM, :] = np.array(\n",
    "                    [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]) # right\n",
    "            else:\n",
    "                hand_lm[HAND_NUM:HAND_NUM * 2, :] = np.array(\n",
    "                    [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]) # left\n",
    "    return hand_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gesture model \n",
    "gest = tf.lite.Interpreter(model_path=\"gesture_model.tflite\")\n",
    "gest.allocate_tensors()\n",
    "gest_input = gest.get_input_details()\n",
    "gest_output = gest.get_output_details()\n",
    "mapping = {\n",
    "    0: 'closedfist',\n",
    "    1: 'four',\n",
    "    2: 'openpalm',\n",
    "    3: 'pointup',\n",
    "    4: 'three',\n",
    "    5: 'thumbsdown',\n",
    "    6: 'thumbsup',\n",
    "    7: 'victory'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloss_mapping_path = \"590_gloss_mapping.json\"\n",
    "index_gloss_mapping_path = \"590_index_gloss_mapping.json\"\n",
    "index_label_mapping_path = \"590_index_label_mapping.json\"\n",
    "\n",
    "gloss_mapping = json.load(open(gloss_mapping_path, \"r\"))\n",
    "index_gloss_mapping = json.load(open(index_gloss_mapping_path, \"r\"))\n",
    "index_label_mapping = json.load(open(index_label_mapping_path, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model.tflite'\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sign(input_data):\n",
    "    input_data = np.expand_dims(input_data, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(interpreter.get_input_details()[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 120, 55, 3], [1, 590])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = list(map(int, interpreter.get_input_details()[0]['shape']))\n",
    "output_shape = list(map(int, interpreter.get_output_details()[0]['shape']))\n",
    "input_shape, output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Live Feed (Sign & Gesture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode Switched to sign\n",
      "Sign : we will see and Gesture : victory\n",
      "Sign : three and Gesture : three\n",
      "Sign : seven and Gesture : thumbsdown\n",
      "Sign : three and Gesture : thumbsdown\n",
      "Mode Switched to gesture\n",
      "Mode Switched to sign\n",
      "Sign : upset and Gesture : thumbsdown\n",
      "Sign : upset and Gesture : thumbsdown\n",
      "Sign : fed up and Gesture : thumbsdown\n",
      "Sign : fed up and Gesture : thumbsdown\n",
      "Sign : fed up and Gesture : thumbsdown\n",
      "Sign : bandage and Gesture : three\n",
      "Sign : fine and Gesture : thumbsdown\n",
      "Sign : fed up and Gesture : thumbsdown\n",
      "Sign : fine and Gesture : three\n",
      "Sign : things and Gesture : three\n",
      "Sign : happy and Gesture : thumbsdown\n",
      "Sign : complain and Gesture : three\n",
      "Sign : problem and Gesture : thumbsdown\n",
      "Sign : bad and Gesture : three\n",
      "Sign : late and Gesture : thumbsdown\n",
      "Sign : fed up and Gesture : thumbsdown\n",
      "Sign : bad and Gesture : thumbsdown\n",
      "Sign : fed up and Gesture : thumbsdown\n"
     ]
    }
   ],
   "source": [
    "mode = 'gesture'\n",
    "text = ''\n",
    "gesture = ''\n",
    "cap = cv2.VideoCapture(0)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "sequence = deque(maxlen=input_shape[1])\n",
    "for _ in range(input_shape[1]):\n",
    "    sequence.append(np.zeros((input_shape[2], 3)))\n",
    "step_length = 60\n",
    "TIME_PER_STEP = step_length / 30.0\n",
    "step_time = time.time()\n",
    "frame_time = 0\n",
    "step = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: continue\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_rgb.flags.writeable = False\n",
    "    if mode == 'sign':\n",
    "        fps = str(int(1 / (time.time() - frame_time)))\n",
    "        frame_time = time.time()\n",
    "        \n",
    "\n",
    "            \n",
    "        all_landmarks = get_all_landmarks(frame_rgb)\n",
    "        hand_landmarks = all_landmarks[:42,:].reshape(-1,42,3)\n",
    "        gest.set_tensor(gest_input[0]['index'], np.array(hand_landmarks, dtype=np.float32))\n",
    "        gest.invoke()\n",
    "        output_data = gest.get_tensor(gest_output[0]['index'])\n",
    "        predicted_class = np.argmax(output_data)\n",
    "        gesture = mapping[predicted_class]\n",
    "        if gesture == 'thumbsup':\n",
    "            mode = 'gesture'\n",
    "            print(f'Mode Switched to {mode}')\n",
    "        cv2.putText(frame, f'recognised sign is ({text})', (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 240, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame, f'Mode : {mode}', (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 240, 0), 2, cv2.LINE_AA)\n",
    "            \n",
    "        step.append(all_landmarks)\n",
    "    \n",
    "        if time.time() - step_time >= TIME_PER_STEP:\n",
    "            step = np.array(step)\n",
    "            step = np.apply_along_axis(lambda arr: np.interp(np.linspace(0, 1, step_length),\n",
    "                                                             np.linspace(0, 1, arr.shape[0]), arr),\n",
    "                                       axis=0, arr=step)\n",
    "    \n",
    "            \n",
    "            sequence.extend(step)\n",
    "            prediction = predict_sign(np.array(sequence))\n",
    "            prediction = prediction.reshape(-1)\n",
    "            prediction = prediction.argmax()\n",
    "            sign = index_label_mapping[str(prediction)]\n",
    "            print(f'Sign : {sign} and Gesture : {gesture}')\n",
    "            text = sign\n",
    "            step_time = time.time()\n",
    "            step = []\n",
    "    if mode == 'gesture':\n",
    "        hand_landmarks = get_hands_landmarks(frame_rgb).reshape(-1,42,3)\n",
    "        gest.set_tensor(gest_input[0]['index'], np.array(hand_landmarks, dtype=np.float32))\n",
    "        gest.invoke()\n",
    "        output_data = gest.get_tensor(gest_output[0]['index'])\n",
    "        predicted_class = np.argmax(output_data)\n",
    "        gesture = mapping[predicted_class]\n",
    "        text = gesture\n",
    "        if gesture == 'victory':\n",
    "            mode = 'sign'\n",
    "            print(f'Mode Switched to {mode}')\n",
    "            \n",
    "        cv2.putText(frame, f'recognised gesture is ({text})', (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 240, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame, f'Mode : {mode}', (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 240, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "        \n",
    "    cv2.imshow(\"Test\", frame)\n",
    "    cv2.setWindowProperty(\"Test\", cv2.WND_PROP_TOPMOST, 1)\n",
    "    k = cv2.waitKey(1)\n",
    "    if k == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
